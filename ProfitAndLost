import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
# path of chromedriver
path = r"C:\Program Files (x86)\chromedriver.exe"
from selenium.webdriver.chrome.options import Options
# to get selenium faster
chrome_options = Options()
chrome_options.add_argument("--disable-extensions")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--headless")
driver = webdriver.Chrome(path, options=chrome_options)


##

for i in range(100):
    reports = (
                driver.find_element_by_id("tTable")
                .find_element_by_class_name("grid-txt")
                .find_elements_by_xpath(f"//*[@id='template-container']/tr[{i}]/td[4]/a")
            )
    for report in reports:
        url = report.get_attribute("href")
        links.append(url)
for i in range(2,100):
    driver.get(f'https://my.codal.ir/fa/statements/?company_id=&my_basket=&statement_type=146&period=12&financial_years=&company_type=0&status=1&tracing_number=&publisher_state=&title=&from_date=&to_date=&parent_or_subset=1&consolidated_or_not=&per_page=100&page={i}')
    for j in range(100):
        reports = (
                driver.find_element_by_id("tTable")
                .find_element_by_class_name("grid-txt")
                .find_elements_by_xpath(f"//*[@id='template-container']/tr[{j}]/td[4]/a")
            )
        for report in reports:
            url = report.get_attribute("href")
            links.append(url)       
# Profit and Lost
Links = [i + "1" for i in links]
def drivetable(soup, number):

    header = soup.find_all("table")[number].find("tr")
    list_header = []
    for items in header:
        try:
            list_header.append(items.get_text())
        except:
            continue

    # for getting the data
    HTML_data = soup.find_all("table")[number].find_all("tr")[1:]
    data = []
    for element in HTML_data:
        sub_data = []
        for sub_element in element:
            try:
                sub_data.append(sub_element.get_text())
            except:
                continue
        data.append(sub_data)
    df = pd.DataFrame(data=data, columns=list_header).T
    return df


##
path = "C:/Users/H3/Desktop/github/mycodal"
error = []
print(len(Links))


def first_type(link):
    driver.get(link)
    name = driver.find_element_by_id("ctl00_txbSymbol").text
    period = driver.find_element_by_id("ctl00_lblPeriod").text
    date = driver.find_element_by_id("ctl00_lblPeriodEndToDate").text[:4]
    r = driver.page_source
    soup = BeautifulSoup(r, "html.parser")
    st_df = drivetable(soup, 0).T
    return st_df, name, period, date


def sec_type(link):
    driver.get(link)
    name = driver.find_element_by_id("ctl00_txbSymbol").text
    period = driver.find_element_by_id("ctl00_lblPeriod").text
    date = driver.find_element_by_id("ctl00_lblPeriodEndToDate").text[:4]
    table = driver.find_element_by_id(
        "ctl00_cphBody_UpdatePanel1"
    )
    dfbase = pd.DataFrame()
    table = (
        WebDriverWait(driver, 10)
        .until(
            EC.visibility_of_element_located(
                (
                    By.CSS_SELECTOR,
                    "#ctl00_cphBody_UpdatePanel1",
                )
            )
        )
        .get_attribute("outerHTML")
    )
    df = pd.read_html(str(table))[1]
    dfbase = dfbase.append(df, ignore_index=True)
    return dfbase, name, period, date


def gen_file(link):
    driver.get(link)
    r = driver.page_source
    soup = BeautifulSoup(r, "html.parser")
    st_df = drivetable(soup, 0).T
    if len(st_df) < 13:
        df, name, period, date = sec_type(link)
    else:
        df, name, period, date = first_type(link)
    return df, name, period, date

##
# There are two types of data tables
for number, link in enumerate(Links):
    print(number)
    try:
        df, name, period, date = gen_file(link)
        df.to_excel(path + "{}_{}_{}.xlsx".format(period, name, date))
    except:
        error.append(link)

##
import pickle
# save errors to check
pickle.dump(error, open(path + "errors.p", "wb"))
